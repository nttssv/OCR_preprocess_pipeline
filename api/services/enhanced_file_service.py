#!/usr/bin/env python3
"""
Enhanced File Management Service
Integrated file handling with metadata extraction, storage management, and fast retrieval
"""

import os\nimport hashlib\nimport mimetypes\nfrom datetime import datetime\nfrom typing import Optional, Dict, Any, Tuple, List\nfrom pathlib import Path\nimport uuid\n\nfrom PIL import Image, ImageOps\nfrom PyPDF2 import PdfReader\nimport fitz  # PyMuPDF for better PDF handling\n\nfrom ..core.config import settings\nfrom ..core.database import (\n    EnhancedDocumentOps, DocumentPageOps, FileMetadataOps,\n    calculate_file_hash\n)\nfrom ..services.cache_service import cache_service\nfrom ..core.logger import get_logger\n\nlogger = get_logger(__name__)\n\n\nclass EnhancedFileService:\n    \"\"\"Enhanced file service with metadata extraction and storage management\"\"\"\n    \n    def __init__(self):\n        # Ensure directories exist\n        os.makedirs(settings.UPLOAD_DIR, exist_ok=True)\n        os.makedirs(settings.OUTPUT_DIR, exist_ok=True)\n        os.makedirs(settings.TEMP_DIR, exist_ok=True)\n        \n        # Create subdirectories for organized storage\n        self.thumbnails_dir = Path(settings.OUTPUT_DIR) / \"thumbnails\"\n        self.processed_dir = Path(settings.OUTPUT_DIR) / \"processed\"\n        self.originals_dir = Path(settings.OUTPUT_DIR) / \"originals\"\n        \n        for directory in [self.thumbnails_dir, self.processed_dir, self.originals_dir]:\n            directory.mkdir(exist_ok=True)\n    \n    async def process_uploaded_file(\n        self,\n        file_path: str,\n        original_filename: str,\n        db\n    ) -> Tuple[str, Dict[str, Any]]:\n        \"\"\"Process uploaded file with comprehensive metadata extraction\"\"\"\n        \n        try:\n            # Calculate file hash for uniqueness\n            content_hash = calculate_file_hash(file_path)\n            \n            # Get file statistics\n            file_stats = os.stat(file_path)\n            file_size = file_stats.st_size\n            \n            # Detect MIME type\n            mime_type, _ = mimetypes.guess_type(original_filename)\n            file_extension = os.path.splitext(original_filename)[1].lower()\n            \n            # Determine file type\n            if file_extension in ['.pdf']:\n                file_type = 'pdf'\n            elif file_extension in ['.png', '.jpg', '.jpeg', '.tiff', '.tif', '.bmp']:\n                file_type = 'image'\n            else:\n                file_type = 'unknown'\n            \n            # Extract document metadata\n            document_metadata = await self._extract_document_metadata(file_path, file_type)\n            \n            # Create unique filename for storage\n            storage_filename = f\"{uuid.uuid4().hex}_{original_filename}\"\n            \n            # Create document record with comprehensive metadata\n            document = EnhancedDocumentOps.create_document_with_metadata(\n                db=db,\n                filename=storage_filename,\n                original_filename=original_filename,\n                original_path=file_path,\n                file_size=file_size,\n                file_type=file_type,\n                content_hash=content_hash,\n                page_count=document_metadata.get('page_count'),\n                document_dimensions=document_metadata.get('dimensions'),\n                mime_type=mime_type\n            )\n            \n            # Create file metadata record\n            file_metadata = FileMetadataOps.create_metadata(\n                db=db,\n                document_id=document.id,\n                file_format=document_metadata.get('format'),\n                compression=document_metadata.get('compression'),\n                color_space=document_metadata.get('color_space'),\n                bit_depth=document_metadata.get('bit_depth'),\n                creator_software=document_metadata.get('creator'),\n                creation_date=document_metadata.get('creation_date'),\n                has_text=document_metadata.get('has_text', False),\n                has_images=document_metadata.get('has_images', False),\n                custom_metadata=document_metadata.get('custom_metadata', {})\n            )\n            \n            # Pre-generate page records for multi-page documents\n            if document_metadata.get('page_count', 1) > 1:\n                await self._create_page_records(\n                    db, document.id, \n                    file_path, \n                    document_metadata.get('page_count', 1)\n                )\n            \n            # Generate thumbnail for quick preview\n            thumbnail_path = await self._generate_thumbnail(file_path, document.id)\n            \n            # Setup storage paths\n            storage_paths = {\n                \"original\": file_path,\n                \"thumbnail\": thumbnail_path,\n                \"processed_dir\": str(self.processed_dir / document.id)\n            }\n            \n            # Update document with storage paths\n            EnhancedDocumentOps.update_document_storage_paths(\n                db, document.id, storage_paths\n            )\n            \n            logger.info(f\"üìÑ Processed uploaded file: {original_filename} -> {document.id}\")\n            \n            return document.id, {\n                \"document_id\": document.id,\n                \"content_hash\": content_hash,\n                \"file_size\": file_size,\n                \"file_type\": file_type,\n                \"page_count\": document_metadata.get('page_count', 1),\n                \"dimensions\": document_metadata.get('dimensions'),\n                \"storage_paths\": storage_paths,\n                \"thumbnail_available\": bool(thumbnail_path)\n            }\n            \n        except Exception as e:\n            logger.error(f\"‚ùå Error processing uploaded file: {str(e)}\")\n            raise\n    \n    async def _extract_document_metadata(self, file_path: str, file_type: str) -> Dict[str, Any]:\n        \"\"\"Extract comprehensive document metadata\"\"\"\n        metadata = {\n            \"page_count\": 1,\n            \"dimensions\": None,\n            \"format\": None,\n            \"compression\": None,\n            \"color_space\": None,\n            \"bit_depth\": None,\n            \"creator\": None,\n            \"creation_date\": None,\n            \"has_text\": False,\n            \"has_images\": False,\n            \"custom_metadata\": {}\n        }\n        \n        try:\n            if file_type == 'pdf':\n                # Use PyMuPDF for comprehensive PDF metadata\n                doc = fitz.open(file_path)\n                \n                metadata.update({\n                    \"page_count\": doc.page_count,\n                    \"format\": \"PDF\",\n                    \"creator\": doc.metadata.get('creator', ''),\n                    \"has_text\": any(page.get_text().strip() for page in doc),\n                    \"has_images\": any(len(page.get_images()) > 0 for page in doc)\n                })\n                \n                # Get dimensions from first page\n                if doc.page_count > 0:\n                    first_page = doc[0]\n                    rect = first_page.rect\n                    metadata[\"dimensions\"] = {\n                        \"width\": int(rect.width),\n                        \"height\": int(rect.height),\n                        \"unit\": \"points\"\n                    }\n                \n                # Extract creation date\n                if doc.metadata.get('creationDate'):\n                    try:\n                        # Parse PDF date format\n                        date_str = doc.metadata['creationDate']\n                        if date_str.startswith('D:'):\n                            date_str = date_str[2:16]  # Extract YYYYMMDDHHMMSS\n                            metadata[\"creation_date\"] = datetime.strptime(date_str, '%Y%m%d%H%M%S')\n                    except:\n                        pass\n                \n                doc.close()\n                \n            elif file_type == 'image':\n                # Use PIL for image metadata\n                with Image.open(file_path) as img:\n                    metadata.update({\n                        \"format\": img.format,\n                        \"dimensions\": {\n                            \"width\": img.width,\n                            \"height\": img.height,\n                            \"unit\": \"pixels\"\n                        },\n                        \"color_space\": img.mode,\n                        \"has_images\": True\n                    })\n                    \n                    # Get additional image info\n                    if hasattr(img, 'info'):\n                        dpi = img.info.get('dpi')\n                        if dpi:\n                            metadata[\"dimensions\"][\"dpi\"] = dpi[0]  # Usually (x_dpi, y_dpi)\n                        \n                        # Check for compression info\n                        if 'compression' in img.info:\n                            metadata[\"compression\"] = str(img.info['compression'])\n        \n        except Exception as e:\n            logger.warning(f\"‚ö†Ô∏è Error extracting metadata from {file_path}: {str(e)}\")\n        \n        return metadata\n    \n    async def _create_page_records(\n        self, \n        db, \n        document_id: str, \n        file_path: str, \n        page_count: int\n    ):\n        \"\"\"Create page records for multi-page documents\"\"\"\n        \n        try:\n            # For PDF files, extract individual pages\n            if file_path.lower().endswith('.pdf'):\n                doc = fitz.open(file_path)\n                \n                for page_num in range(page_count):\n                    page = doc[page_num]\n                    \n                    # Calculate page hash (based on text content)\n                    page_content = page.get_text()\n                    page_hash = hashlib.md5(page_content.encode()).hexdigest()\n                    \n                    # Get page dimensions\n                    rect = page.rect\n                    page_dimensions = {\n                        \"width\": int(rect.width),\n                        \"height\": int(rect.height),\n                        \"unit\": \"points\"\n                    }\n                    \n                    # Create page record\n                    DocumentPageOps.create_page(\n                        db=db,\n                        document_id=document_id,\n                        page_number=page_num + 1,  # 1-based numbering\n                        page_hash=page_hash,\n                        page_dimensions=page_dimensions\n                    )\n                \n                doc.close()\n                logger.info(f\"üìÑ Created {page_count} page records for document {document_id}\")\n        \n        except Exception as e:\n            logger.error(f\"‚ùå Error creating page records: {str(e)}\")\n    \n    async def _generate_thumbnail(\n        self, \n        file_path: str, \n        document_id: str,\n        size: Tuple[int, int] = (200, 200)\n    ) -> Optional[str]:\n        \"\"\"Generate thumbnail for quick preview\"\"\"\n        \n        try:\n            thumbnail_filename = f\"{document_id}_thumbnail.png\"\n            thumbnail_path = self.thumbnails_dir / thumbnail_filename\n            \n            if file_path.lower().endswith('.pdf'):\n                # Generate thumbnail from first page of PDF\n                doc = fitz.open(file_path)\n                if doc.page_count > 0:\n                    page = doc[0]\n                    # Render page as image\n                    mat = fitz.Matrix(1.0, 1.0)  # Identity matrix\n                    pix = page.get_pixmap(matrix=mat)\n                    \n                    # Convert to PIL Image\n                    img_data = pix.tobytes(\"png\")\n                    with open(thumbnail_path, \"wb\") as f:\n                        f.write(img_data)\n                    \n                    # Resize to thumbnail size\n                    with Image.open(thumbnail_path) as img:\n                        img.thumbnail(size, Image.Resampling.LANCZOS)\n                        img.save(thumbnail_path, \"PNG\")\n                \n                doc.close()\n            \n            else:\n                # Generate thumbnail from image file\n                with Image.open(file_path) as img:\n                    # Convert to RGB if necessary\n                    if img.mode != 'RGB':\n                        img = img.convert('RGB')\n                    \n                    # Create thumbnail\n                    img.thumbnail(size, Image.Resampling.LANCZOS)\n                    img.save(thumbnail_path, \"PNG\")\n            \n            logger.info(f\"üñºÔ∏è Generated thumbnail: {thumbnail_path}\")\n            return str(thumbnail_path)\n        \n        except Exception as e:\n            logger.error(f\"‚ùå Error generating thumbnail: {str(e)}\")\n            return None\n    \n    async def get_page_image_path(\n        self, \n        db, \n        document_id: str, \n        page_number: int,\n        image_type: str = \"processed\"\n    ) -> Optional[str]:\n        \"\"\"Get page image path with caching for O(1) retrieval\"\"\"\n        \n        try:\n            # Check cache first\n            cache_key = f\"{document_id}:{page_number}:{image_type}\"\n            cached_path = await cache_service.get_page_info(document_id, page_number)\n            \n            if cached_path and image_type in cached_path.get('paths', {}):\n                path = cached_path['paths'][image_type]\n                if path and os.path.exists(path):\n                    return path\n            \n            # Get from database\n            page = DocumentPageOps.get_page(db, document_id, page_number)\n            if not page:\n                return None\n            \n            # Select appropriate path\n            if image_type == \"thumbnail\" and page.thumbnail_path:\n                return page.thumbnail_path\n            elif image_type == \"processed\" and page.processed_image_path:\n                return page.processed_image_path\n            elif image_type == \"original\" and page.original_image_path:\n                return page.original_image_path\n            \n            return None\n        \n        except Exception as e:\n            logger.error(f\"‚ùå Error getting page image path: {str(e)}\")\n            return None\n    \n    async def update_page_processing_result(\n        self,\n        db,\n        document_id: str,\n        page_number: int,\n        processed_image_path: str,\n        processing_time: float,\n        quality_metrics: Optional[Dict[str, Any]] = None\n    ) -> bool:\n        \"\"\"Update page with processing results\"\"\"\n        \n        try:\n            # Get page record\n            page = DocumentPageOps.get_page(db, document_id, page_number)\n            if not page:\n                logger.error(f\"‚ùå Page not found: {document_id}:{page_number}\")\n                return False\n            \n            # Calculate quality score if metrics provided\n            quality_score = None\n            is_blank = False\n            \n            if quality_metrics:\n                # Simple quality scoring based on available metrics\n                quality_score = quality_metrics.get('sharpness_score', 0.5)\n                is_blank = quality_metrics.get('is_blank', False)\n            \n            # Update page record\n            DocumentPageOps.update_page_paths(\n                db, page.id, processed_path=processed_image_path\n            )\n            \n            DocumentPageOps.update_page_processing_status(\n                db, \n                page.id,\n                status=\"completed\",\n                processing_time=processing_time,\n                quality_score=quality_score,\n                is_blank=is_blank,\n                processing_metadata=quality_metrics\n            )\n            \n            # Invalidate cache\n            await cache_service.invalidate_document(document_id)\n            \n            logger.info(f\"‚úÖ Updated processing result for page {page_number} of document {document_id}\")\n            return True\n        \n        except Exception as e:\n            logger.error(f\"‚ùå Error updating page processing result: {str(e)}\")\n            return False\n    \n    async def search_documents_by_content_hash(\n        self, \n        db, \n        content_hash: str\n    ) -> List[Dict[str, Any]]:\n        \"\"\"Search for documents with the same content hash (duplicates)\"\"\"\n        \n        try:\n            documents = EnhancedDocumentOps.search_by_hash(db, content_hash)\n            \n            results = []\n            for doc in documents:\n                doc_info = doc.to_dict()\n                \n                # Add additional metadata if available\n                file_metadata = FileMetadataOps.get_metadata(db, doc.id)\n                if file_metadata:\n                    doc_info['processing_analytics'] = {\n                        'average_processing_time': file_metadata.average_processing_time,\n                        'total_operations': file_metadata.total_processing_operations\n                    }\n                \n                results.append(doc_info)\n            \n            return results\n        \n        except Exception as e:\n            logger.error(f\"‚ùå Error searching by content hash: {str(e)}\")\n            return []\n    \n    def get_storage_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get storage usage statistics\"\"\"\n        \n        stats = {\n            \"directories\": {\n                \"uploads\": self._get_directory_stats(settings.UPLOAD_DIR),\n                \"processed\": self._get_directory_stats(self.processed_dir),\n                \"thumbnails\": self._get_directory_stats(self.thumbnails_dir),\n                \"originals\": self._get_directory_stats(self.originals_dir),\n                \"cache\": self._get_directory_stats(settings.CACHE_DIR)\n            },\n            \"total_files\": 0,\n            \"total_size_mb\": 0.0\n        }\n        \n        # Calculate totals\n        for dir_stats in stats[\"directories\"].values():\n            stats[\"total_files\"] += dir_stats[\"file_count\"]\n            stats[\"total_size_mb\"] += dir_stats[\"size_mb\"]\n        \n        return stats\n    \n    def _get_directory_stats(self, directory: str) -> Dict[str, Any]:\n        \"\"\"Get statistics for a directory\"\"\"\n        \n        try:\n            dir_path = Path(directory)\n            if not dir_path.exists():\n                return {\"file_count\": 0, \"size_mb\": 0.0, \"exists\": False}\n            \n            files = list(dir_path.rglob(\"*\"))\n            file_count = len([f for f in files if f.is_file()])\n            total_size = sum(f.stat().st_size for f in files if f.is_file())\n            \n            return {\n                \"file_count\": file_count,\n                \"size_mb\": total_size / (1024 * 1024),\n                \"exists\": True,\n                \"path\": str(dir_path)\n            }\n        \n        except Exception:\n            return {\"file_count\": 0, \"size_mb\": 0.0, \"exists\": False}\n\n\n# Global service instance\nenhanced_file_service = EnhancedFileService()